{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a7d7050b-a87a-4954-adf9-afb9cf6d058a",
      "metadata": {
        "id": "a7d7050b-a87a-4954-adf9-afb9cf6d058a"
      },
      "source": [
        "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
        "\n",
        "#### Name: Prithvi Vadlamani\n",
        "#### Stevens ID: 10476457"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe7ac70-1da0-4432-839d-6d65ce4edf35",
      "metadata": {
        "id": "bfe7ac70-1da0-4432-839d-6d65ce4edf35"
      },
      "source": [
        "## Part A: Multi-Layer Perceptron (MLP) (50 Points)\n",
        "\n",
        "## In this assignment, you are required to follow the steps below:\n",
        "1. Review the lecture slides.\n",
        "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
        "3. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
        "4. Implement the MLP with Tensorflow and compare to your implementation.\n",
        "5. Analysis the results in the Conlusion part.\n",
        "\n",
        "**Before you start**\n",
        "- Please read the code very carefully.\n",
        "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
        "```console\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
        "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
        "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b95a75c8-30c8-4ca6-ac9d-42c90d32b04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b95a75c8-30c8-4ca6-ac9d-42c90d32b04d",
        "outputId": "84b65c8b-19bc-43f7-d5bd-4dc7fb77d822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (3.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.8.2+zzzcolab20220929150707)\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.18.0)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (5.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (7.9.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: nbclassic in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (4.11.1)\n",
            "Requirement already satisfied: jupyter-server~=1.16 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (1.21.0)\n",
            "Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (6.2)\n",
            "Requirement already satisfied: jupyterlab-server~=2.10 in /usr/local/lib/python3.7/dist-packages (from jupyterlab->-r requirements.txt (line 1)) (2.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (5.4.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (3.6.1)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (7.2.1)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (0.13.3)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (5.6.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (0.14.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (21.3.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (5.0.0)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (4.3.3)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.10.3)\n",
            "Requirement already satisfied: json5 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (0.9.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (5.9.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (5.0.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (4.6.3)\n",
            "Requirement already satisfied: mistune<3,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (2.0.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.5.0->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.5.6)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.2.0->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (2.16.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 3)) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->-r requirements.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (0.5.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (14.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (0.27.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (22.9.24)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.49.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 6)) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons->-r requirements.txt (line 7)) (2.7.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->jupyterlab->-r requirements.txt (line 1)) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.5)\n",
            "Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from nbclassic->jupyterlab->-r requirements.txt (line 1)) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "# you may not run this cell after the first installation\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91d76782-187b-4da7-baf2-d92655c79d63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91d76782-187b-4da7-baf2-d92655c79d63",
        "outputId": "d9c7f88c-483f-42a1-d71d-96d921fce3c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63231e67-b83e-4628-9c49-31b6d096b4f7",
      "metadata": {
        "id": "63231e67-b83e-4628-9c49-31b6d096b4f7"
      },
      "source": [
        "## 1. Data Processing (5 points)\n",
        "\n",
        "* Download the dataset from Canvas\n",
        "* Load data to text and labels\n",
        "* Preprocessing\n",
        "* Tokenization\n",
        "* Split data\n",
        "* Feature extraction (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d184b3d1-7b4b-4ee0-85b9-12a205f58687",
      "metadata": {
        "id": "d184b3d1-7b4b-4ee0-85b9-12a205f58687"
      },
      "source": [
        "#### Download NLTK stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "636b2ba9-1e79-4479-b1d8-28e139590cf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "636b2ba9-1e79-4479-b1d8-28e139590cf2",
        "outputId": "4864f5ab-b6a8-4ce1-8f8c-01ef56d1fe71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to a2-data/nltk...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk_path = os.path.join('a2-data', 'nltk')\n",
        "nltk.download('stopwords', download_dir=nltk_path)\n",
        "nltk.data.path.append(nltk_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a98cc310-65bb-49dc-bd54-476f4e941f7d",
      "metadata": {
        "id": "a98cc310-65bb-49dc-bd54-476f4e941f7d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "def print_line(*args):\n",
        "    \"\"\" Inline print and go to the begining of line\n",
        "    \"\"\"\n",
        "    args1 = [str(arg) for arg in args]\n",
        "    str_ = ' '.join(args1)\n",
        "    sys.stdout.write(str_ + '\\r')\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "653997a8-637a-4725-8ee5-f240913e0644",
      "metadata": {
        "id": "653997a8-637a-4725-8ee5-f240913e0644"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union,Dict\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9eb08e4-91fa-4d1e-bc92-0445397567ca",
      "metadata": {
        "id": "b9eb08e4-91fa-4d1e-bc92-0445397567ca"
      },
      "source": [
        "### 1.1 Load data\n",
        "\n",
        "- Load sentences and labels\n",
        "- Transform string labels into integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9fc26198-cfbf-4c2f-89e6-460f5277c177",
      "metadata": {
        "id": "9fc26198-cfbf-4c2f-89e6-460f5277c177"
      },
      "outputs": [],
      "source": [
        "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\" Load sentences and labels from the specified path\n",
        "    Args:\n",
        "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
        "        sentences: the raw text list of all sentences\n",
        "    Returns:\n",
        "        labels: the label list of all sentences\n",
        "    \"\"\"\n",
        "    sentences, labels = [], []\n",
        "    # Start your code here (load text and label from files)\n",
        "    corpus = open(\"books.txt\")\n",
        "\n",
        "    for sent in corpus:\n",
        "      line = sent.split(\"\\t\")\n",
        "      labels.append(line[0])\n",
        "      sentences.append(line[1])\n",
        "    # End\n",
        "    return sentences, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "103a0975-aaa9-48ac-9a2b-445757ef614b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "103a0975-aaa9-48ac-9a2b-445757ef614b",
        "outputId": "746cf82e-6788-477a-a5fb-d8ec8d813a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
            "Number of sentences and labels: 19536 19536\n"
          ]
        }
      ],
      "source": [
        "data_path = os.path.join('a2-data', 'books.txt')\n",
        "sentences, labels = load_sentence_label(data_path)\n",
        "\n",
        "label_map = {}\n",
        "for label in sorted(list(set(labels))):\n",
        "    label_map[label] = len(label_map)\n",
        "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
        "sentences = np.array(sentences, dtype=object)\n",
        "\n",
        "print('Label map:', label_map)\n",
        "print('Number of sentences and labels:', len(sentences), len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20468e5-9fb9-48ec-ae47-92b463549853",
      "metadata": {
        "id": "b20468e5-9fb9-48ec-ae47-92b463549853"
      },
      "source": [
        "#### Split the data into training, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dba74813-eb7f-484c-bc9a-0f391ae48a09",
      "metadata": {
        "id": "dba74813-eb7f-484c-bc9a-0f391ae48a09"
      },
      "outputs": [],
      "source": [
        "def train_test_split(sentences: np.ndarray,\n",
        "                     labels: np.ndarray,\n",
        "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
        "    Args:\n",
        "        sentences: A numpy array containing all sentences\n",
        "        labels: A number array containing label ids\n",
        "        test_ratio: A float number to calculate the number of test data\n",
        "\n",
        "    Returns:\n",
        "        train_sentences: A numpy array containing all training sentences\n",
        "        train_labels: A number array containing all training label ids\n",
        "        test_sentences: A numpy array containing all test sentences\n",
        "        test_labels: A number array containing all test label ids\n",
        "    \"\"\"\n",
        "    assert 0 < test_ratio < 1\n",
        "    assert len(sentences) == len(labels)\n",
        "\n",
        "    train_index, test_index = [], []\n",
        "    # Start your code here (split the index for training and test)\n",
        "\n",
        "    length = len(sentences)\n",
        "    n_test = int(np.ceil(length*test_ratio))\n",
        "    n_train = length - n_test\n",
        "    \n",
        "    perm = np.random.permutation(length)\n",
        "    test_index = perm[:n_test]\n",
        "    train_index = perm[n_test:]\n",
        "\n",
        "    # End\n",
        "\n",
        "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
        "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
        "    return train_sentences, train_labels, test_sentences, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1757acef-5872-4683-86da-c4ae68af7f18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1757acef-5872-4683-86da-c4ae68af7f18",
        "outputId": "b1fa9b53-33cf-482f-bbe1-843842ec0a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 14065\n",
            "Validation data length: 1563\n",
            "Test data length: 3908\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(6666)\n",
        "\n",
        "test_ratio = 0.2\n",
        "valid_ratio = 0.1\n",
        "(train_sentences, train_labels,\n",
        "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
        "(train_sentences, train_labels,\n",
        "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
        "\n",
        "print('Training data length:', len(train_sentences))\n",
        "print('Validation data length:', len(valid_sentences))\n",
        "print('Test data length:', len(test_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f1202227-1197-4c6c-b1c4-525bcbb65ea9",
      "metadata": {
        "id": "f1202227-1197-4c6c-b1c4-525bcbb65ea9"
      },
      "outputs": [],
      "source": [
        "def count_label(labels: np.ndarray, label_map: Dict[str, int]) -> Dict[str, int]:\n",
        "    \n",
        "    \"\"\"\n",
        "    Args:\n",
        "        labels: The labels of a dataset \n",
        "        label_map: The mapping from label to label id\n",
        "    Returns:\n",
        "        label_count: The mapping from label to its count\n",
        "    \"\"\"\n",
        "    \n",
        "    label_count = {key: 0 for key in label_map.keys()}\n",
        "    \n",
        "    # Start your code here (count the number of each label)\n",
        "\n",
        "    label_count = {a: list(labels).count(a) for a in labels}\n",
        "\n",
        "    # End\n",
        "    return label_count    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e7744c12-8055-478a-b64b-67104fd81c92",
      "metadata": {
        "id": "e7744c12-8055-478a-b64b-67104fd81c92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef99d0dd-db59-4324-e5e9-6c495f990e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: {0: 1871, 1: 4239, 2: 7955}\n",
            "Validation: {2: 912, 1: 459, 0: 192}\n",
            "Test: {2: 2187, 1: 1246, 0: 475}\n"
          ]
        }
      ],
      "source": [
        "print('Training:', count_label(train_labels, label_map))\n",
        "print('Validation:', count_label(valid_labels, label_map))\n",
        "print('Test:', count_label(test_labels, label_map))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47476cac-f0da-4b92-aa09-8f57e61053e9",
      "metadata": {
        "id": "47476cac-f0da-4b92-aa09-8f57e61053e9"
      },
      "source": [
        "#### Dataset statistics\n",
        "Fill this table with the statistics you just printed (double click this cell to edit)\n",
        "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
        "|:--------------:|--------------------|--------------------|-------------|-------|\n",
        "|  **Training**  |        1871        |        4239        |     7955    |       |\n",
        "| **Validation** |         192        |         459        |      912       |       |\n",
        "|    **Test**    |         475        |        1246        |     2187    |       |\n",
        "|    **Total**   |        2538        |        5944        |    11054    |       |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8438e6-64a6-4355-b39f-88bdf3550593",
      "metadata": {
        "id": "ff8438e6-64a6-4355-b39f-88bdf3550593"
      },
      "source": [
        "### 1.2 Preprocess\n",
        "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1007c276-5692-4f3b-afd8-d2be06f6ecfd",
      "metadata": {
        "id": "1007c276-5692-4f3b-afd8-d2be06f6ecfd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, punctuation=True, url=True, number=True):\n",
        "        self.punctuation = punctuation\n",
        "        self.url = url\n",
        "        self.number = number\n",
        "\n",
        "    def apply(self, sentence: str) -> str:\n",
        "        \"\"\" Apply the preprocessing rules to the sentence\n",
        "        Args:\n",
        "            sentence: raw sentence\n",
        "        Returns:\n",
        "            sentence: clean sentence\n",
        "        \"\"\"\n",
        "        sentence = sentence.lower()\n",
        "        if self.url:\n",
        "            sentence = Preprocessor.remove_url(sentence)\n",
        "        if self.punctuation:\n",
        "            sentence = Preprocessor.remove_punctuation(sentence)\n",
        "        if self.number:\n",
        "            sentence = Preprocessor.remove_number(sentence)\n",
        "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_punctuation(sentence: str) -> str:\n",
        "        \"\"\" Remove punctuations in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible punctuations\n",
        "        Returns:\n",
        "            sentence: sentence without punctuations\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        sentence = re.sub(pattern = \"[^\\w\\s]\",\n",
        "              repl = \"\",\n",
        "              string = sentence)\n",
        "        # End\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_url(sentence: str) -> str:\n",
        "        \"\"\" Remove urls in text with re\n",
        "        Args:\n",
        "            sentence: sentence with possible urls\n",
        "        Returns:\n",
        "            sentence: sentence without urls\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        sentence = re.sub(pattern = r\"http\\S+\", \n",
        "                         repl = \"\",\n",
        "                         string = sentence)\n",
        "        # End\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_number(sentence: str) -> str:\n",
        "        \"\"\" Remove numbers in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible numbers\n",
        "        Returns:\n",
        "            sentence: sentence without numbers\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        sentence = re.sub(pattern = \"[0-9]*\",\n",
        "                         repl = \"\",\n",
        "                         string = sentence)\n",
        "        # End\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c56c4e51-e434-4703-a23e-bb609c0d3b51",
      "metadata": {
        "id": "c56c4e51-e434-4703-a23e-bb609c0d3b51"
      },
      "source": [
        "##### Test your implementation by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bbf21295-1e18-46ab-b9fe-07fba8d9e4c0",
      "metadata": {
        "id": "bbf21295-1e18-46ab-b9fe-07fba8d9e4c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c39f328-f827-43d6-d8a3-2a75aa1df843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
            "===>\n",
            "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
        "\n",
        "processor = Preprocessor()\n",
        "clean_sentence = processor.apply(sentence)\n",
        "\n",
        "print(f'\"{sentence}\"') \n",
        "print('===>')\n",
        "print(f'\"{clean_sentence}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e173a8d-8f57-40e6-b278-052290ab9ffe",
      "metadata": {
        "id": "9e173a8d-8f57-40e6-b278-052290ab9ffe"
      },
      "source": [
        "### 1.3 Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "facf2964-b766-4a2a-8709-ac831c041b30",
      "metadata": {
        "id": "facf2964-b766-4a2a-8709-ac831c041b30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26aa807-31c1-4aa4-ff77-0c8885418236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['against', 'don', 'our', 'there', 'under', 'can', 'my', 'more', 'you', 'did']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "print(list(stopwords_set)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e3fc31ba-ee84-4268-9f7c-f15fe7169151",
      "metadata": {
        "id": "e3fc31ba-ee84-4268-9f7c-f15fe7169151"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence: str) -> List[str]:\n",
        "    \"\"\" Tokenize a sentence into tokens (words)\n",
        "    Args:\n",
        "        sentence: clean sentence\n",
        "    Returns:\n",
        "        tokens\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    # Start your code here\n",
        "    #     Step 1. Split sentence into words\n",
        "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
        "    #     Step 3. Remove stop words using the defined stopwords_set\n",
        "    sentence = \" \".join(sentence.split()) # Split\n",
        "    notes = sentence.split(\" \")\n",
        "    \n",
        "    rootWords = [] # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    for w in notes:\n",
        "        rootWord = ps.stem(w)\n",
        "        rootWords.append(rootWord)\n",
        "        \n",
        "    stopWords = set(stopwords.words('english'))  # Stop words\n",
        "    for i in rootWords:\n",
        "        if i not in stopWords:\n",
        "            words.append(i)\n",
        "    # End\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bfc568d-4343-42c5-9c5b-c4e4627ebae6",
      "metadata": {
        "id": "9bfc568d-4343-42c5-9c5b-c4e4627ebae6"
      },
      "source": [
        "##### Test your implementation by running the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f1ea9a53-a8f5-4f46-b30e-5116714adcf0",
      "metadata": {
        "id": "f1ea9a53-a8f5-4f46-b30e-5116714adcf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0feccf7a-1924-4f93-e013-510eacf2306e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
            "===>\n",
            "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
        "\n",
        "processor = Preprocessor()\n",
        "clean_sentence = processor.apply(sentence)\n",
        "tokens = tokenize(clean_sentence)\n",
        "\n",
        "print(f'\"{sentence}\"') \n",
        "print('===>')\n",
        "print(f'\"{tokens}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccea260-2e4c-45e6-8d07-953452fd8c4b",
      "metadata": {
        "id": "7ccea260-2e4c-45e6-8d07-953452fd8c4b"
      },
      "source": [
        "### 1.5 Feature Extraction\n",
        "\n",
        "TF-IDF:\n",
        "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
        "\n",
        "- $t$: A term\n",
        "- $d$: A document. Here, we regard a sentence as a document\n",
        "- $f_{t, d}$: Number of term $t$ in $d$\n",
        "- $N$: Number of document\n",
        "- $n_t$: Number of document containing $t$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fc82e419-05d8-4919-8b97-194b2933171a",
      "metadata": {
        "id": "fc82e419-05d8-4919-8b97-194b2933171a"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class TfIdfEncoder:\n",
        "    def __init__(self):\n",
        "        self.vocab = defaultdict(int)\n",
        "        self.token2index = {}\n",
        "        self.df = defaultdict(int)\n",
        "        self.num_doc = 0\n",
        "        self.processor = Preprocessor()\n",
        "\n",
        "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
        "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
        "            In this function, you are required to implement the fitting process.\n",
        "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
        "                2. Construct the document frequency map to tokens (self.df).\n",
        "                3. Construct the token to index map based on the frequency.\n",
        "                   The token with a higher frequency has the smaller index\n",
        "        Args:\n",
        "            sentences: Raw sentences\n",
        "        Returns:\n",
        "            token_num\n",
        "        \"\"\"\n",
        "        self.num_doc = len(sentences)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == len(sentences) - 1:\n",
        "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
        "            # Start your code here (step 1 & 2)\n",
        "\n",
        "            self.processor = Preprocessor()\n",
        "            clean_sentence = self.processor.apply(sentence)\n",
        "            tokens = tokenize(clean_sentence)\n",
        "\n",
        "            for word in tokens:\n",
        "                if word in self.vocab:\n",
        "                    self.vocab[word] += 1\n",
        "                else:\n",
        "                    self.vocab[word] = 1\n",
        "                    \n",
        "                    \n",
        "            tokens_set = set(tokens)\n",
        "            \n",
        "            for word in tokens_set:\n",
        "                if word in self.df:\n",
        "                    self.df[word] +=1\n",
        "                else:\n",
        "                    self.df[word] = 1\n",
        "\n",
        "            # End\n",
        "        print_line('\\n')\n",
        "        # Start your code here (Step 3)\n",
        "\n",
        "        self.token2index = dict(sorted(self.vocab.items()), key = lambda x:x[1], reverse=True) #{token:highest_freq}\n",
        "        i = 0\n",
        "        for key, value in self.token2index.items():\n",
        "            self.token2index[key] = i\n",
        "            i+=1    \n",
        "\n",
        "        # End\n",
        "        token_num = len(self.token2index) \n",
        "        print('The number of distinct tokens:', token_num)\n",
        "        return token_num\n",
        "\n",
        "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
        "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
        "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
        "        Args:\n",
        "            sentences: Raw sentences\n",
        "        Returns:\n",
        "            features: A (n x token_num) matrix, where n is the number of sentences\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        features = np.zeros((n, len(self.token2index)))\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
        "            # Start your code (calculate TF-IDF)\n",
        "\n",
        "            tfDict = {}\n",
        "            idfDict = {}\n",
        "            bowCount = len(sentence)\n",
        "            \n",
        "            for word in sentence:\n",
        "                if word in tfDict:\n",
        "                    tfDict[word] +=1\n",
        "                else:\n",
        "                    tfDict[word] = 1\n",
        "                    \n",
        "                tf = tfDict[word]/float(len(sentence))\n",
        "                \n",
        "                #IDF\n",
        "                try:\n",
        "                    idf = np.log(n/self.df[word])\n",
        "                except:\n",
        "                    idf = np.log(n)\n",
        "                \n",
        "                tfIDF = tf*idf\n",
        "                \n",
        "                if word in self.token2index:\n",
        "                    features[i][self.token2index[word]] = tfIDF\n",
        "                    \n",
        "            # End\n",
        "        print_line('\\n')\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61755eeb-5d1c-4923-b58a-0e9409f4b4cd",
      "metadata": {
        "id": "61755eeb-5d1c-4923-b58a-0e9409f4b4cd"
      },
      "source": [
        "##### Test your implementation by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f1c1bd2b-4d06-46f2-97da-9ce53fb714ec",
      "metadata": {
        "id": "f1c1bd2b-4d06-46f2-97da-9ce53fb714ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bb8144-81eb-4cf8-a64e-65aca19b6805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The number of distinct tokens: 1335\n",
            "\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "encoder = TfIdfEncoder()\n",
        "encoder.fit(train_sentences[:100])\n",
        "features = encoder.encode(train_sentences[:10])\n",
        "\n",
        "print(features[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6e3946-8032-474c-9643-74ee3c5fcdb9",
      "metadata": {
        "id": "1f6e3946-8032-474c-9643-74ee3c5fcdb9"
      },
      "source": [
        "#### Encode training, validation, and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "85fd6854-727c-4cfd-bb66-e080f21dd909",
      "metadata": {
        "id": "85fd6854-727c-4cfd-bb66-e080f21dd909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe090648-ab52-4302-ea9c-f1feb68e8ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The number of distinct tokens: 16825\n",
            "\n",
            "\n",
            "\n",
            "The size of training set: (14065, 16825) (14065, 3)\n",
            "The size of validation set: (1563, 16825) (1563, 3)\n",
            "The size of test set: (3908, 16825) (3908, 3)\n"
          ]
        }
      ],
      "source": [
        "num_class = 3\n",
        "\n",
        "encoder = TfIdfEncoder()\n",
        "vocab_size = encoder.fit(train_sentences)\n",
        "\n",
        "x_train = encoder.encode(train_sentences)\n",
        "x_valid = encoder.encode(valid_sentences)\n",
        "x_test = encoder.encode(test_sentences)\n",
        "\n",
        "y_train = np.zeros((len(train_labels), num_class))\n",
        "y_valid = np.zeros((len(valid_labels), num_class))\n",
        "y_test = np.zeros((len(test_labels), num_class))\n",
        "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
        "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
        "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
        "\n",
        "print('The size of training set:', x_train.shape, y_train.shape)\n",
        "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
        "print('The size of test set:', x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e71bd1-478c-45e6-b679-6def46ff4829",
      "metadata": {
        "id": "11e71bd1-478c-45e6-b679-6def46ff4829"
      },
      "source": [
        "## 2. MLP (20 Points)\n",
        "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
        "\n",
        "The objective function of LR for multi-class classification:\n",
        "\n",
        "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
        "\n",
        "- $z_1 = w_1x$\n",
        "- $h_1 = activation(z_1)$\n",
        "- $z_2 = w_2 h_1$\n",
        "- $\\hat{y} = softmax(z_2)$\n",
        "\n",
        "- $n$: Number of samples\n",
        "- $d$: Dimension of $\\mathbf{w}$\n",
        "- Here, you can use `sigmoid` as the activation function for the hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "944a5d09-483d-4fed-b8f8-6b5da594c59a",
      "metadata": {
        "id": "944a5d09-483d-4fed-b8f8-6b5da594c59a"
      },
      "source": [
        "### 2.1 MLP Model (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "44537480-4186-488f-b4e7-89d211054717",
      "metadata": {
        "id": "44537480-4186-488f-b4e7-89d211054717"
      },
      "outputs": [],
      "source": [
        "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
        "    \"\"\" The softmax activation function\n",
        "    Args:\n",
        "        x: Input matrix or vector\n",
        "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
        "    Returns:\n",
        "        output: Softmax value of the specified dimension in x\n",
        "    \"\"\"\n",
        "    # Start your code here\n",
        "    # e = np.exp(x-np.max(x))\n",
        "\n",
        "    # for i in range(len(x)):\n",
        "    #   e[i] /= np.sum(e[i])\n",
        "    num = x - np.max(x, axis = axis).reshape(x.shape[0],1)\n",
        "    x = np.exp(num) / np.sum(np.exp(x), axis = axis).reshape(x.shape[0],1)\n",
        "    # End\n",
        "    return x\n",
        "\n",
        "\n",
        "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" The sigmoid activation function\n",
        "    Args:\n",
        "        x: Input matrix or vector\n",
        "    Returns:\n",
        "        output: Sigmoid value of each entry in x\n",
        "    \"\"\"\n",
        "    # Start your code here\n",
        "    x = 1/(1+np.exp(-x))\n",
        "    # End\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e8fefa08-6231-4fbf-869a-d07ebce350f8",
      "metadata": {
        "id": "e8fefa08-6231-4fbf-869a-d07ebce350f8"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
        "        \"\"\" MLP Model\n",
        "        Args:\n",
        "            feature_dim: feature dimension\n",
        "            hidden_dim: hidden units\n",
        "            num_class: number of class\n",
        "            lambda_: lambda in L2 regularizer\n",
        "        \"\"\"\n",
        "        # Start your code here (initialize weight and bias)\n",
        "        # self.w1 =\n",
        "        # self.b1 =\n",
        "        # self.w2 =\n",
        "        # self.b2 =\n",
        "        self.w1 = np.random.uniform(size = (feature_dim, hidden_dim))\n",
        "        self.b1 = np.random.uniform(size = (1, hidden_dim))\n",
        "\n",
        "        self.w2 = np.random.uniform(size = (hidden_dim, num_class))\n",
        "        self.b2 = np.random.uniform(size = (1, num_class))\n",
        "        # End\n",
        "        self.lambda_ = lambda_\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, x: np.ndarray, return_hiddens: bool = False) -> np.ndarray:\n",
        "        \"\"\" Forward process of logistic regression\n",
        "            Calculate y_hat using x\n",
        "        Args:\n",
        "            x: Input data\n",
        "            return_hiddens: If true the function will return h1 for gradient calculation\n",
        "        Returns:\n",
        "            y_hat: Output\n",
        "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
        "        \"\"\"\n",
        "        y_hat = 0\n",
        "        h1 = 0, 0\n",
        "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
        "        # Start your code here (calculate y_hat of MLP using x)\n",
        "\n",
        "        h1 = sigmoid(np.dot(x, self.w1) + self.b1)\n",
        "        y_hat = softmax(np.dot(h1, self.w2) + self.b2, axis=1)        \n",
        "\n",
        "        # End\n",
        "        if return_hiddens:\n",
        "            return y_hat, h1\n",
        "        else:\n",
        "            return y_hat\n",
        "\n",
        "    def backward(self,\n",
        "                 x: np.ndarray,\n",
        "                 y_hat: np.ndarray,\n",
        "                 y: np.ndarray,\n",
        "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
        "        \"\"\" Backward process of logistic regression\n",
        "            Calculate the gradient of w and b\n",
        "        Args:\n",
        "            x: Input data\n",
        "            y_hat: Output of forward\n",
        "            y: Ground-truth\n",
        "            h1: Hidden output of the hidden layer\n",
        "        Returns:\n",
        "            dw1: Gradient of w1\n",
        "            db1: Gradient of b1\n",
        "            dw2: Gradient of w2\n",
        "            db2: Gradient of b2\n",
        "        \"\"\"\n",
        "        w1, w2 = self.w1, self.w2\n",
        "        dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
        "        n = len(x)\n",
        "\n",
        "        # Start your code here (calculate the gradient of w and b)\n",
        "\n",
        "        dw2 = (1/n) * np.dot(h1.T, (y_hat - y)) + (self.lambda_ * np.sum(2 * w2))\n",
        "        # db2 = (1/ n) * np.sum(y_hat - y).T.reshape(1,-1) #(3,) -> (,3) -> #(1,3)\n",
        "        db2 = (1/n)*(np.sum((y_hat - y), axis = 0))  \n",
        "\n",
        "        #dw1 = (1/n) * np.dot(np.dot(np.dot(h1,(1-h1).T),x).T, np.dot(y_hat-y,w2.T)) + (self.lambda_ * np.sum(2 * w1))\n",
        "        #db1 = (1/n) * np.sum(np.dot(np.dot(y_hat-y,w2.T).T,np.dot(h1,(1-h1).T)),axis=1).reshape(1,-1)\n",
        "        \n",
        "        a = np.dot((y_hat - y), w2.T)\n",
        "        b = np.multiply(a, h1) \n",
        "        c = np.multiply(b,(1-h1)) \n",
        "        d = np.dot(x.T,c)\n",
        "\n",
        "        dw1 = (1/n) * d \n",
        "        db1 = (1/n) * np.sum(c, axis=0) \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        # End\n",
        "        return dw1, db1, dw2, db2\n",
        "\n",
        "    def categorical_cross_entropy_loss(self,\n",
        "                                       y_hat: np.ndarray,\n",
        "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
        "        \"\"\" Calculate the binary cross-entropy loss\n",
        "        Args:\n",
        "            y_hat: Output of forward\n",
        "            y: Ground-truth\n",
        "        Returns:\n",
        "            loss: BCE loss\n",
        "        \"\"\"\n",
        "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
        "        loss = 0\n",
        "        # Start your code here (Calculate the binary cross-entropy)\n",
        "        loss = -np.mean(y*np.log(y_hat)) + self.lambda_*np.sum((self.w2)**2)\n",
        "        # End\n",
        "        return loss\n",
        "\n",
        "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
        "        #print(self.b1.shape, db1.shape)\n",
        "        self.w1 -= lr * dw1\n",
        "        self.b1 -= lr * db1\n",
        "        self.w2 -= lr * dw2\n",
        "        self.b2 -= lr * db2\n",
        "\n",
        "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
        "        \"\"\" Predict the label using the output y_hat\n",
        "        Args:\n",
        "            y_hat: Model output\n",
        "        Returns:\n",
        "            pred: Prediction\n",
        "        \"\"\"\n",
        "        pred = np.zeros_like(y_hat)\n",
        "        index = np.argmax(y_hat, axis=-1)\n",
        "        pred[np.arange(len(y_hat)), index] = 1\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edf807c-3c04-42df-9ad6-3aa8bbc02e38",
      "metadata": {
        "id": "0edf807c-3c04-42df-9ad6-3aa8bbc02e38"
      },
      "source": [
        "### 2.2 Evaluation Metrics\n",
        "\n",
        "Accuracy, Precision, Recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5f627c34-e233-4313-82b3-d5285556209b",
      "metadata": {
        "id": "5f627c34-e233-4313-82b3-d5285556209b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
        "\n",
        "\n",
        "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
        "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
        "    Args:\n",
        "        y_pred: Prediction\n",
        "        y_true: Ground-truth\n",
        "    Returns:\n",
        "        accuracy: float number. The accuracy for the whole dataset\n",
        "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
        "    \"\"\"\n",
        "    assert y_pred.shape == y_true.shape\n",
        "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
        "    # Start your code here\n",
        "    #precision, recall, f1 = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
        "    TN = TP = FN = FP = 0\n",
        "    \"\"\"\n",
        "    for i in range(len(y_pred[0])):        \n",
        "        \n",
        "        if y_pred[i] == 0 and y_true[i] == 0:\n",
        "            TN += 1\n",
        "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
        "            FP += 1\n",
        "        elif y_pred[i] == 1 and y_true[i] == 0:\n",
        "            FN += 1\n",
        "        elif y_pred[i] == 1 and y_true[i] == 1:\n",
        "            TP += 1\n",
        "            \n",
        "            \n",
        "    accuracy = accuracy/(len(y_pred[0]))\n",
        "\n",
        "    try:\n",
        "        precision = (TP)/(FP + TP)\n",
        "        recall = (TP)/(FN + TP)\n",
        "    \n",
        "        f1 = (2*precision*recall)/(precision + recall)\n",
        "        \n",
        "    except:\n",
        "        precision = recall = f1 = 0\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average = 'micro')\n",
        "        \n",
        "        \n",
        "    # End\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1794868a-0c46-4219-8d82-979c79e6fedb",
      "metadata": {
        "id": "1794868a-0c46-4219-8d82-979c79e6fedb"
      },
      "source": [
        "### 2.3 AdaGrad (5 points)\n",
        "\n",
        "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
        "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4a817e4d-7f0f-429a-84a6-03c1bdd50680",
      "metadata": {
        "id": "4a817e4d-7f0f-429a-84a6-03c1bdd50680"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "    def __init__(self, init_lr, model):\n",
        "        self.init_lr = init_lr\n",
        "        self.model = model\n",
        "        \n",
        "        self.accumulative_dw1 = 0\n",
        "        self.accumulative_db1 = 0\n",
        "        self.accumulative_dw2 = 0\n",
        "        self.accumulative_db2 = 0\n",
        "        self.eps = 1e-9\n",
        "        \n",
        "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
        "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
        "            2. Calculate the new gradient with the accumulative gradient\n",
        "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
        "        \n",
        "        Do not return anything\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        # Step 1\n",
        "        #print(np.square(db1).shape)\n",
        "        self.accumulative_dw1 += np.square(dw1)\n",
        "        self.accumulative_dw2 += np.square(dw2)\n",
        "        self.accumulative_db1 += np.square(db1)\n",
        "        self.accumulative_db2 += np.square(db2)        \n",
        "        \n",
        "        # Step 2\n",
        "        \n",
        "        dw1 = dw1/(np.sqrt(self.accumulative_dw1 + self.eps))\n",
        "        dw2 = dw2/(np.sqrt(self.accumulative_dw2 + self.eps))\n",
        "        db1 = db1/(np.sqrt(self.accumulative_db1 + self.eps))\n",
        "        db2 = db2/(np.sqrt(self.accumulative_db2 + self.eps))\n",
        "\n",
        "        # Step 3\n",
        "\n",
        "        self.model.gradient_descent(dw1, db1, dw2, db2, self.init_lr)\n",
        "\n",
        "        # End\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e976ba-17d9-45ac-818e-fbfdf304b190",
      "metadata": {
        "id": "31e976ba-17d9-45ac-818e-fbfdf304b190"
      },
      "source": [
        "### 2.4 Mini-batch Gradient Descent (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "958ee97d-7c88-4a23-8da5-c43fa36038e6",
      "metadata": {
        "id": "958ee97d-7c88-4a23-8da5-c43fa36038e6"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def train_mbgd(model: 'MLP',\n",
        "               x_train: np.ndarray,\n",
        "               y_train: np.ndarray,\n",
        "               x_valid: np.ndarray,\n",
        "               y_valid: np.ndarray,\n",
        "               lr: float,\n",
        "               num_epoch: int,\n",
        "               batch_size: int,\n",
        "               print_every: int = 10) -> Tuple[Dict[str, List], Dict[str, List]]:\n",
        "    \"\"\" Training with Gradient Descent\n",
        "    Args:\n",
        "        model: The logistic regression model\n",
        "        x_train: Training feature, (n x d) matrix\n",
        "        y_train: Training label, (n, ) vector\n",
        "        x_valid: Validation feature, (n x d) matrix\n",
        "        y_valid: Validation label, (n, ) vector\n",
        "        lr: Learning rate\n",
        "        num_epoch: Number of training epochs\n",
        "        batch_size: Number of training samples in a batch\n",
        "        print_every: Print log every {print_every} epochs\n",
        "    Returns:\n",
        "        train_history: Log of training information. The format of training history is\n",
        "                       { 'loss': [] }\n",
        "                       It records the average loss of each epoch.\n",
        "        valid_history: Log of validation information. The format of training and validation history is\n",
        "                       {\n",
        "                           'loss': [],\n",
        "                           'accuracy': [],\n",
        "                           'precision': [],\n",
        "                           'recall': [],\n",
        "                           'f1': []\n",
        "                       }\n",
        "    \"\"\"\n",
        "    train_history = OrderedDict({'loss': []})\n",
        "    valid_history = OrderedDict({\n",
        "        'loss': [],\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1': []\n",
        "    })\n",
        "\n",
        "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
        "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
        "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
        "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
        "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
        "        return log\n",
        "\n",
        "    # IMPORTANT: YOU SHOULD USE THIS OPTIMIZER TO UPDATE THE MODEL\n",
        "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
        "\n",
        "    train_num_samples = len(x_train)\n",
        "    n_batch = train_num_samples // batch_size\n",
        "    for epoch in range(num_epoch):\n",
        "        epoch_loss = 0.0\n",
        "        # Start your code here (training)\n",
        "        #     Step 1. Model forward\n",
        "        #     Step 2. Calculate loss\n",
        "        #     Step 3. Model backward\n",
        "        #     Step 4. Optimization with Adagrad\n",
        "\n",
        "        for i in range(n_batch):\n",
        "            #x_batch = x_train[(i*batch_size) : (i*batch_size)+((i*batch_size)+batch_size)]\n",
        "            #y_batch = y_train[(i*batch_size) : (i*batch_size)+((i*batch_size)+batch_size)]\n",
        "            x_batch = x_train[i : i+batch_size]\n",
        "            y_batch = y_train[i : i+batch_size]\n",
        "            y_hat,h1 = model.forward(x_batch, return_hiddens = True)\n",
        "            epoch_loss += model.categorical_cross_entropy_loss(y_hat, y_batch)\n",
        "            dw1, db1, dw2, db2 = model.backward(x_batch, y_hat, y_batch, h1)\n",
        "            #model.gradient_descent(model.dw1, model.db1, model.dw2, model.db2, lr)\n",
        "            optimizer.update(dw1, db1, dw2, db2)\n",
        "        #epoch_loss = epoch_loss/n_batch\n",
        "\n",
        "        \n",
        "\n",
        "        # End\n",
        "\n",
        "        valid_loss = 0.\n",
        "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
        "        # Start your code here (validation)\n",
        "        #     Step 1. Predict\n",
        "        #     Step 2. Calculate loss\n",
        "        #     Step 3. Calculate metrics\n",
        "\n",
        "        y_hat = model.forward(x_valid, return_hiddens = False)\n",
        "        y_hat = model.predict(y_hat)\n",
        "        valid_loss = model.categorical_cross_entropy_loss(y_hat, y_valid)\n",
        "        accuracy, precision, recall, f1 = get_metrics(y_hat, y_valid)\n",
        "\n",
        "        # End\n",
        "\n",
        "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
        "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
        "            vals.append(val)\n",
        "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
        "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
        "            print(log)\n",
        "        else:\n",
        "            print_line(log)\n",
        "\n",
        "    return train_history, valid_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9273e6-eda3-4a16-8779-810bdc55cf33",
      "metadata": {
        "id": "5f9273e6-eda3-4a16-8779-810bdc55cf33"
      },
      "source": [
        "Run Mini-batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "49874274-247f-43c5-973c-5d8917869b08",
      "metadata": {
        "id": "49874274-247f-43c5-973c-5d8917869b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543a8da1-5868-4fe6-d1c8-382c1fe2b7d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 11 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 21 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 31 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 41 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 51 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 61 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 71 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 81 / 100: train_loss: 0.0535 - valid_loss: 2.8771 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 91 / 100: train_loss: 0.0535 - valid_loss: 2.8772 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
            "Epoch 100 / 100: train_loss: 0.0535 - valid_loss: 2.8772 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(6666)\n",
        "\n",
        "hidden_dim = 128\n",
        "num_epoch = 100\n",
        "lr = 1e-2\n",
        "batch_size = 128\n",
        "lambda_ = 1e-8\n",
        "print_every = 10\n",
        "\n",
        "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
        "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b3020d5-8254-4dce-8034-56be5a5ecb19",
      "metadata": {
        "id": "6b3020d5-8254-4dce-8034-56be5a5ecb19"
      },
      "source": [
        "### 2.5 MLP using Tensorflow (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "bc1bbb3f-8ec7-4582-b590-7bc5b1155f1b",
      "metadata": {
        "id": "bc1bbb3f-8ec7-4582-b590-7bc5b1155f1b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Softmax\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "\n",
        "\n",
        "class MLPTF(Model):\n",
        "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
        "        \"\"\" MLP Model using tensorflow.keras\n",
        "        Args:\n",
        "            feature_dim: feature dimension\n",
        "            hidden_dim: hidden units\n",
        "            num_class: number of class\n",
        "            lambda_: lambda in L2 regularizer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Start your code here (initialize weight and bias)\n",
        "        # self.dense1 =\n",
        "        # self.dense2 =\n",
        "        # self.softmax = \n",
        "        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L2(lambda_), use_bias=True, bias_regularizer =tf.keras.regularizers.L2(lambda_), activity_regularizer= tf.keras.regularizers.L2(lambda_))\n",
        "        self.dense2 = tf.keras.layers.Dense(num_class, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(lambda_), use_bias=True, bias_regularizer =tf.keras.regularizers.L2(lambda_), activity_regularizer= tf.keras.regularizers.L2(lambda_))\n",
        "        # End\n",
        "        \n",
        "    def call(self, x):\n",
        "        \"\"\" Forward function of tf. It should be named 'call'\n",
        "        \n",
        "        Args:\n",
        "            x: (n x feature_dim) tensor\n",
        "        Returns:\n",
        "            y_hat: (n x num_class) tensor\n",
        "        \"\"\"\n",
        "        # Start your code here (Forward)\n",
        "        h1 = self.dense1(x)\n",
        "        y_hat = self.dense2(h1)\n",
        "        # End\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "879b1fea-4852-4b0e-994a-9637982717a9",
      "metadata": {
        "id": "879b1fea-4852-4b0e-994a-9637982717a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590d5b05-bfc7-466c-d2b3-50d21781a274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"mlptf\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               multiple                  2153728   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,154,115\n",
            "Trainable params: 2,154,115\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_addons as tfa\n",
        "np.random.seed(6666)\n",
        "tf.random.set_seed(6666)\n",
        "\n",
        "\n",
        "hidden_dim = 128\n",
        "num_epoch = 100\n",
        "lr = 1e-1\n",
        "batch_size = 128\n",
        "lambda_ = 1e-8\n",
        "\n",
        "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
        "model_tf.build(input_shape=(None, vocab_size))\n",
        "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
        "model_tf.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1541100f-3e5e-4024-82eb-3e2a6d7d3f7a",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "1541100f-3e5e-4024-82eb-3e2a6d7d3f7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c66faf2-a3a7-41e7-b38b-833a44d1b7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "110/110 [==============================] - 5s 10ms/step - loss: 1.0153 - categorical_accuracy: 0.5381 - f1_score: 0.5381 - val_loss: 0.9371 - val_categorical_accuracy: 0.5835 - val_f1_score: 0.5835\n",
            "Epoch 2/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.9664 - categorical_accuracy: 0.5583 - f1_score: 0.5583 - val_loss: 0.9794 - val_categorical_accuracy: 0.5835 - val_f1_score: 0.5835\n",
            "Epoch 3/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.9631 - categorical_accuracy: 0.5571 - f1_score: 0.5571 - val_loss: 0.9311 - val_categorical_accuracy: 0.5835 - val_f1_score: 0.5835\n",
            "Epoch 4/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9551 - categorical_accuracy: 0.5621 - f1_score: 0.5621 - val_loss: 0.9344 - val_categorical_accuracy: 0.5835 - val_f1_score: 0.5835\n",
            "Epoch 5/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9506 - categorical_accuracy: 0.5646 - f1_score: 0.5646 - val_loss: 0.9628 - val_categorical_accuracy: 0.5643 - val_f1_score: 0.5643\n",
            "Epoch 6/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9491 - categorical_accuracy: 0.5634 - f1_score: 0.5634 - val_loss: 0.9327 - val_categorical_accuracy: 0.5841 - val_f1_score: 0.5841\n",
            "Epoch 7/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9471 - categorical_accuracy: 0.5652 - f1_score: 0.5652 - val_loss: 0.9529 - val_categorical_accuracy: 0.5681 - val_f1_score: 0.5681\n",
            "Epoch 8/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9454 - categorical_accuracy: 0.5615 - f1_score: 0.5615 - val_loss: 0.9223 - val_categorical_accuracy: 0.5835 - val_f1_score: 0.5835\n",
            "Epoch 9/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9424 - categorical_accuracy: 0.5616 - f1_score: 0.5616 - val_loss: 0.9319 - val_categorical_accuracy: 0.5835 - val_f1_score: 0.5835\n",
            "Epoch 10/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9419 - categorical_accuracy: 0.5615 - f1_score: 0.5615 - val_loss: 0.9279 - val_categorical_accuracy: 0.5841 - val_f1_score: 0.5841\n",
            "Epoch 11/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9398 - categorical_accuracy: 0.5632 - f1_score: 0.5632 - val_loss: 0.9210 - val_categorical_accuracy: 0.5854 - val_f1_score: 0.5854\n",
            "Epoch 12/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9386 - categorical_accuracy: 0.5619 - f1_score: 0.5619 - val_loss: 0.9202 - val_categorical_accuracy: 0.5854 - val_f1_score: 0.5854\n",
            "Epoch 13/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9360 - categorical_accuracy: 0.5616 - f1_score: 0.5616 - val_loss: 0.9230 - val_categorical_accuracy: 0.5854 - val_f1_score: 0.5854\n",
            "Epoch 14/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9348 - categorical_accuracy: 0.5620 - f1_score: 0.5620 - val_loss: 0.9200 - val_categorical_accuracy: 0.5880 - val_f1_score: 0.5880\n",
            "Epoch 15/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9335 - categorical_accuracy: 0.5621 - f1_score: 0.5621 - val_loss: 0.9209 - val_categorical_accuracy: 0.5886 - val_f1_score: 0.5886\n",
            "Epoch 16/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9327 - categorical_accuracy: 0.5624 - f1_score: 0.5624 - val_loss: 0.9155 - val_categorical_accuracy: 0.5873 - val_f1_score: 0.5873\n",
            "Epoch 17/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9314 - categorical_accuracy: 0.5633 - f1_score: 0.5633 - val_loss: 0.9397 - val_categorical_accuracy: 0.5758 - val_f1_score: 0.5758\n",
            "Epoch 18/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9309 - categorical_accuracy: 0.5633 - f1_score: 0.5633 - val_loss: 0.9207 - val_categorical_accuracy: 0.5841 - val_f1_score: 0.5841\n",
            "Epoch 19/100\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.9288 - categorical_accuracy: 0.5642 - f1_score: 0.5642 - val_loss: 0.9119 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
            "Epoch 20/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9300 - categorical_accuracy: 0.5665 - f1_score: 0.5665 - val_loss: 0.9160 - val_categorical_accuracy: 0.5886 - val_f1_score: 0.5886\n",
            "Epoch 21/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9275 - categorical_accuracy: 0.5643 - f1_score: 0.5643 - val_loss: 0.9354 - val_categorical_accuracy: 0.5848 - val_f1_score: 0.5848\n",
            "Epoch 22/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9281 - categorical_accuracy: 0.5660 - f1_score: 0.5660 - val_loss: 0.9101 - val_categorical_accuracy: 0.5886 - val_f1_score: 0.5886\n",
            "Epoch 23/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9264 - categorical_accuracy: 0.5672 - f1_score: 0.5672 - val_loss: 0.9122 - val_categorical_accuracy: 0.5873 - val_f1_score: 0.5873\n",
            "Epoch 24/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9258 - categorical_accuracy: 0.5693 - f1_score: 0.5693 - val_loss: 0.9299 - val_categorical_accuracy: 0.5797 - val_f1_score: 0.5797\n",
            "Epoch 25/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9255 - categorical_accuracy: 0.5711 - f1_score: 0.5711 - val_loss: 0.9323 - val_categorical_accuracy: 0.5816 - val_f1_score: 0.5816\n",
            "Epoch 26/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9238 - categorical_accuracy: 0.5702 - f1_score: 0.5702 - val_loss: 0.9081 - val_categorical_accuracy: 0.5880 - val_f1_score: 0.5880\n",
            "Epoch 27/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9256 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9084 - val_categorical_accuracy: 0.5880 - val_f1_score: 0.5880\n",
            "Epoch 28/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9222 - categorical_accuracy: 0.5721 - f1_score: 0.5721 - val_loss: 0.9145 - val_categorical_accuracy: 0.5752 - val_f1_score: 0.5752\n",
            "Epoch 29/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9224 - categorical_accuracy: 0.5724 - f1_score: 0.5724 - val_loss: 0.9166 - val_categorical_accuracy: 0.5745 - val_f1_score: 0.5745\n",
            "Epoch 30/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9224 - categorical_accuracy: 0.5733 - f1_score: 0.5733 - val_loss: 0.9063 - val_categorical_accuracy: 0.5886 - val_f1_score: 0.5886\n",
            "Epoch 31/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9205 - categorical_accuracy: 0.5743 - f1_score: 0.5743 - val_loss: 0.9231 - val_categorical_accuracy: 0.5752 - val_f1_score: 0.5752\n",
            "Epoch 32/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9212 - categorical_accuracy: 0.5757 - f1_score: 0.5757 - val_loss: 0.9218 - val_categorical_accuracy: 0.5765 - val_f1_score: 0.5765\n",
            "Epoch 33/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9203 - categorical_accuracy: 0.5763 - f1_score: 0.5763 - val_loss: 0.9151 - val_categorical_accuracy: 0.5905 - val_f1_score: 0.5905\n",
            "Epoch 34/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9217 - categorical_accuracy: 0.5749 - f1_score: 0.5749 - val_loss: 0.9115 - val_categorical_accuracy: 0.5937 - val_f1_score: 0.5937\n",
            "Epoch 35/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9206 - categorical_accuracy: 0.5738 - f1_score: 0.5738 - val_loss: 0.9042 - val_categorical_accuracy: 0.5912 - val_f1_score: 0.5912\n",
            "Epoch 36/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9190 - categorical_accuracy: 0.5791 - f1_score: 0.5791 - val_loss: 0.9173 - val_categorical_accuracy: 0.5790 - val_f1_score: 0.5790\n",
            "Epoch 37/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9192 - categorical_accuracy: 0.5791 - f1_score: 0.5791 - val_loss: 0.9246 - val_categorical_accuracy: 0.5790 - val_f1_score: 0.5790\n",
            "Epoch 38/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9193 - categorical_accuracy: 0.5767 - f1_score: 0.5767 - val_loss: 0.9101 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 39/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9182 - categorical_accuracy: 0.5819 - f1_score: 0.5819 - val_loss: 0.9099 - val_categorical_accuracy: 0.5918 - val_f1_score: 0.5918\n",
            "Epoch 40/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9190 - categorical_accuracy: 0.5800 - f1_score: 0.5800 - val_loss: 0.9099 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 41/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9177 - categorical_accuracy: 0.5800 - f1_score: 0.5800 - val_loss: 0.9155 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 42/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9178 - categorical_accuracy: 0.5796 - f1_score: 0.5796 - val_loss: 0.9127 - val_categorical_accuracy: 0.5976 - val_f1_score: 0.5976\n",
            "Epoch 43/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9161 - categorical_accuracy: 0.5809 - f1_score: 0.5809 - val_loss: 0.9302 - val_categorical_accuracy: 0.5861 - val_f1_score: 0.5861\n",
            "Epoch 44/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9164 - categorical_accuracy: 0.5806 - f1_score: 0.5806 - val_loss: 0.9080 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 45/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9165 - categorical_accuracy: 0.5831 - f1_score: 0.5831 - val_loss: 0.9031 - val_categorical_accuracy: 0.5912 - val_f1_score: 0.5912\n",
            "Epoch 46/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9154 - categorical_accuracy: 0.5829 - f1_score: 0.5829 - val_loss: 0.9041 - val_categorical_accuracy: 0.5956 - val_f1_score: 0.5956\n",
            "Epoch 47/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9151 - categorical_accuracy: 0.5835 - f1_score: 0.5835 - val_loss: 0.9025 - val_categorical_accuracy: 0.5956 - val_f1_score: 0.5956\n",
            "Epoch 48/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9153 - categorical_accuracy: 0.5835 - f1_score: 0.5835 - val_loss: 0.9066 - val_categorical_accuracy: 0.5950 - val_f1_score: 0.5950\n",
            "Epoch 49/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9153 - categorical_accuracy: 0.5835 - f1_score: 0.5835 - val_loss: 0.9079 - val_categorical_accuracy: 0.6020 - val_f1_score: 0.6020\n",
            "Epoch 50/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9153 - categorical_accuracy: 0.5839 - f1_score: 0.5839 - val_loss: 0.9101 - val_categorical_accuracy: 0.6014 - val_f1_score: 0.6014\n",
            "Epoch 51/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9157 - categorical_accuracy: 0.5845 - f1_score: 0.5845 - val_loss: 0.9044 - val_categorical_accuracy: 0.5963 - val_f1_score: 0.5963\n",
            "Epoch 52/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9151 - categorical_accuracy: 0.5831 - f1_score: 0.5831 - val_loss: 0.9047 - val_categorical_accuracy: 0.5963 - val_f1_score: 0.5963\n",
            "Epoch 53/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9148 - categorical_accuracy: 0.5863 - f1_score: 0.5863 - val_loss: 0.9000 - val_categorical_accuracy: 0.5944 - val_f1_score: 0.5944\n",
            "Epoch 54/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9139 - categorical_accuracy: 0.5868 - f1_score: 0.5868 - val_loss: 0.9102 - val_categorical_accuracy: 0.6046 - val_f1_score: 0.6046\n",
            "Epoch 55/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9140 - categorical_accuracy: 0.5856 - f1_score: 0.5856 - val_loss: 0.9016 - val_categorical_accuracy: 0.5963 - val_f1_score: 0.5963\n",
            "Epoch 56/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9135 - categorical_accuracy: 0.5843 - f1_score: 0.5843 - val_loss: 0.8988 - val_categorical_accuracy: 0.5950 - val_f1_score: 0.5950\n",
            "Epoch 57/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9136 - categorical_accuracy: 0.5861 - f1_score: 0.5861 - val_loss: 0.9010 - val_categorical_accuracy: 0.5925 - val_f1_score: 0.5925\n",
            "Epoch 58/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9137 - categorical_accuracy: 0.5872 - f1_score: 0.5872 - val_loss: 0.9013 - val_categorical_accuracy: 0.5925 - val_f1_score: 0.5925\n",
            "Epoch 59/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9135 - categorical_accuracy: 0.5871 - f1_score: 0.5871 - val_loss: 0.9060 - val_categorical_accuracy: 0.6020 - val_f1_score: 0.6020\n",
            "Epoch 60/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9138 - categorical_accuracy: 0.5850 - f1_score: 0.5850 - val_loss: 0.9043 - val_categorical_accuracy: 0.5988 - val_f1_score: 0.5988\n",
            "Epoch 61/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9130 - categorical_accuracy: 0.5862 - f1_score: 0.5862 - val_loss: 0.8998 - val_categorical_accuracy: 0.5982 - val_f1_score: 0.5982\n",
            "Epoch 62/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9121 - categorical_accuracy: 0.5901 - f1_score: 0.5901 - val_loss: 0.9051 - val_categorical_accuracy: 0.5956 - val_f1_score: 0.5956\n",
            "Epoch 63/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9128 - categorical_accuracy: 0.5839 - f1_score: 0.5839 - val_loss: 0.9016 - val_categorical_accuracy: 0.5963 - val_f1_score: 0.5963\n",
            "Epoch 64/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9126 - categorical_accuracy: 0.5869 - f1_score: 0.5869 - val_loss: 0.9031 - val_categorical_accuracy: 0.6020 - val_f1_score: 0.6020\n",
            "Epoch 65/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9122 - categorical_accuracy: 0.5889 - f1_score: 0.5889 - val_loss: 0.9014 - val_categorical_accuracy: 0.6008 - val_f1_score: 0.6008\n",
            "Epoch 66/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9127 - categorical_accuracy: 0.5872 - f1_score: 0.5872 - val_loss: 0.8976 - val_categorical_accuracy: 0.5944 - val_f1_score: 0.5944\n",
            "Epoch 67/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9127 - categorical_accuracy: 0.5886 - f1_score: 0.5886 - val_loss: 0.9211 - val_categorical_accuracy: 0.5937 - val_f1_score: 0.5937\n",
            "Epoch 68/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9115 - categorical_accuracy: 0.5898 - f1_score: 0.5898 - val_loss: 0.9178 - val_categorical_accuracy: 0.6040 - val_f1_score: 0.6040\n",
            "Epoch 69/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9116 - categorical_accuracy: 0.5891 - f1_score: 0.5891 - val_loss: 0.8999 - val_categorical_accuracy: 0.5937 - val_f1_score: 0.5937\n",
            "Epoch 70/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9127 - categorical_accuracy: 0.5897 - f1_score: 0.5897 - val_loss: 0.8972 - val_categorical_accuracy: 0.5982 - val_f1_score: 0.5982\n",
            "Epoch 71/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9122 - categorical_accuracy: 0.5900 - f1_score: 0.5900 - val_loss: 0.8973 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 72/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9107 - categorical_accuracy: 0.5885 - f1_score: 0.5885 - val_loss: 0.9010 - val_categorical_accuracy: 0.6008 - val_f1_score: 0.6008\n",
            "Epoch 73/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9112 - categorical_accuracy: 0.5905 - f1_score: 0.5905 - val_loss: 0.8982 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 74/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9121 - categorical_accuracy: 0.5873 - f1_score: 0.5873 - val_loss: 0.8971 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 75/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9128 - categorical_accuracy: 0.5883 - f1_score: 0.5883 - val_loss: 0.8961 - val_categorical_accuracy: 0.5982 - val_f1_score: 0.5982\n",
            "Epoch 76/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9113 - categorical_accuracy: 0.5905 - f1_score: 0.5905 - val_loss: 0.9104 - val_categorical_accuracy: 0.6078 - val_f1_score: 0.6078\n",
            "Epoch 77/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9114 - categorical_accuracy: 0.5891 - f1_score: 0.5891 - val_loss: 0.9002 - val_categorical_accuracy: 0.6014 - val_f1_score: 0.6014\n",
            "Epoch 78/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9113 - categorical_accuracy: 0.5898 - f1_score: 0.5898 - val_loss: 0.9113 - val_categorical_accuracy: 0.6072 - val_f1_score: 0.6072\n",
            "Epoch 79/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9118 - categorical_accuracy: 0.5908 - f1_score: 0.5908 - val_loss: 0.9012 - val_categorical_accuracy: 0.5982 - val_f1_score: 0.5982\n",
            "Epoch 80/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9106 - categorical_accuracy: 0.5898 - f1_score: 0.5898 - val_loss: 0.8962 - val_categorical_accuracy: 0.6001 - val_f1_score: 0.6001\n",
            "Epoch 81/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9109 - categorical_accuracy: 0.5911 - f1_score: 0.5911 - val_loss: 0.8976 - val_categorical_accuracy: 0.5976 - val_f1_score: 0.5976\n",
            "Epoch 82/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9106 - categorical_accuracy: 0.5895 - f1_score: 0.5895 - val_loss: 0.9017 - val_categorical_accuracy: 0.6014 - val_f1_score: 0.6014\n",
            "Epoch 83/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9109 - categorical_accuracy: 0.5907 - f1_score: 0.5907 - val_loss: 0.8990 - val_categorical_accuracy: 0.6014 - val_f1_score: 0.6014\n",
            "Epoch 84/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9108 - categorical_accuracy: 0.5892 - f1_score: 0.5892 - val_loss: 0.9000 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 85/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9107 - categorical_accuracy: 0.5888 - f1_score: 0.5888 - val_loss: 0.9017 - val_categorical_accuracy: 0.6046 - val_f1_score: 0.6046\n",
            "Epoch 86/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9105 - categorical_accuracy: 0.5898 - f1_score: 0.5898 - val_loss: 0.8946 - val_categorical_accuracy: 0.5995 - val_f1_score: 0.5995\n",
            "Epoch 87/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9108 - categorical_accuracy: 0.5883 - f1_score: 0.5883 - val_loss: 0.9048 - val_categorical_accuracy: 0.6052 - val_f1_score: 0.6052\n",
            "Epoch 88/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9108 - categorical_accuracy: 0.5898 - f1_score: 0.5898 - val_loss: 0.9143 - val_categorical_accuracy: 0.6072 - val_f1_score: 0.6072\n",
            "Epoch 89/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9117 - categorical_accuracy: 0.5895 - f1_score: 0.5895 - val_loss: 0.9070 - val_categorical_accuracy: 0.6052 - val_f1_score: 0.6052\n",
            "Epoch 90/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9097 - categorical_accuracy: 0.5905 - f1_score: 0.5905 - val_loss: 0.8942 - val_categorical_accuracy: 0.5988 - val_f1_score: 0.5988\n",
            "Epoch 91/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9114 - categorical_accuracy: 0.5886 - f1_score: 0.5886 - val_loss: 0.8954 - val_categorical_accuracy: 0.5988 - val_f1_score: 0.5988\n",
            "Epoch 92/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9103 - categorical_accuracy: 0.5906 - f1_score: 0.5906 - val_loss: 0.8973 - val_categorical_accuracy: 0.5995 - val_f1_score: 0.5995\n",
            "Epoch 93/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9108 - categorical_accuracy: 0.5893 - f1_score: 0.5893 - val_loss: 0.9005 - val_categorical_accuracy: 0.6008 - val_f1_score: 0.6008\n",
            "Epoch 94/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9100 - categorical_accuracy: 0.5913 - f1_score: 0.5913 - val_loss: 0.9055 - val_categorical_accuracy: 0.5982 - val_f1_score: 0.5982\n",
            "Epoch 95/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9100 - categorical_accuracy: 0.5918 - f1_score: 0.5918 - val_loss: 0.8951 - val_categorical_accuracy: 0.5969 - val_f1_score: 0.5969\n",
            "Epoch 96/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9103 - categorical_accuracy: 0.5896 - f1_score: 0.5896 - val_loss: 0.8954 - val_categorical_accuracy: 0.5982 - val_f1_score: 0.5982\n",
            "Epoch 97/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9100 - categorical_accuracy: 0.5905 - f1_score: 0.5905 - val_loss: 0.8943 - val_categorical_accuracy: 0.6008 - val_f1_score: 0.6008\n",
            "Epoch 98/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9108 - categorical_accuracy: 0.5902 - f1_score: 0.5902 - val_loss: 0.9009 - val_categorical_accuracy: 0.6052 - val_f1_score: 0.6052\n",
            "Epoch 99/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9092 - categorical_accuracy: 0.5909 - f1_score: 0.5909 - val_loss: 0.8953 - val_categorical_accuracy: 0.5982 - val_f1_score: 0.5982\n",
            "Epoch 100/100\n",
            "110/110 [==============================] - 1s 8ms/step - loss: 0.9097 - categorical_accuracy: 0.5913 - f1_score: 0.5913 - val_loss: 0.8954 - val_categorical_accuracy: 0.5976 - val_f1_score: 0.5976\n"
          ]
        }
      ],
      "source": [
        "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05bdbd8-bd67-49b5-938a-a71d3dba105e",
      "metadata": {
        "id": "f05bdbd8-bd67-49b5-938a-a71d3dba105e"
      },
      "source": [
        "#### Evaluation with Tensroflow\n",
        "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "eb577c9f-97b6-4984-8264-7aa811fec827",
      "metadata": {
        "id": "eb577c9f-97b6-4984-8264-7aa811fec827",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6f852e-4032-4004-c71e-02cce363ef8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini-batch GD: (0.5596212896622313, 0.5596212896622313, 0.5596212896622313, 0.5596212896622313)\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 0.9049 - categorical_accuracy: 0.5796 - f1_score: 0.5796\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9048570990562439, 0.5795803666114807, 0.5795803666114807]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Calculate the metrics for test set and fill in the table below\n",
        "y_hat = model_mbgd.forward(x_test)\n",
        "y_pred = model_mbgd.predict(y_hat)\n",
        "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
        "model_tf.evaluate(x=x_test, y=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "436dfacd-2ad2-41fa-8f23-f4cc86142fe0",
      "metadata": {
        "id": "436dfacd-2ad2-41fa-8f23-f4cc86142fe0"
      },
      "source": [
        "#### Evaluation Metrics on Test set\n",
        "Fill this table with the result you just printed (double click this cell to edit)\n",
        "|     Optimizer                     | Accuracy    | F1 Score    |\n",
        "|:---------------------------------:|-------------|-------------|\n",
        "|      **Your Implementation**      |   59.13     |    59.13    |\n",
        "| **Tensorflow**                    |   57.96     |    57.96    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fbbb31b-5452-4788-87fc-5d5c7eab144a",
      "metadata": {
        "id": "0fbbb31b-5452-4788-87fc-5d5c7eab144a"
      },
      "source": [
        "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "02501623-a585-426f-8e65-fb14704eb13c",
      "metadata": {
        "id": "02501623-a585-426f-8e65-fb14704eb13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "20f4bc39-8ccb-4e59-ff3f-aae9e2277595"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(4, 3))\n",
        "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
        "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8c5a95-45ab-49d6-b570-25534d15b3b2",
      "metadata": {
        "id": "1b8c5a95-45ab-49d6-b570-25534d15b3b2"
      },
      "source": [
        "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "100f123d-2796-4e48-b204-912bad8c13e6",
      "metadata": {
        "id": "100f123d-2796-4e48-b204-912bad8c13e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "d0c8ce87-1e1c-4258-e2fb-0002a93f0f30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
        "ax = axes[0]\n",
        "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
        "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_title('Validation Accuracy')\n",
        "\n",
        "ax = axes[1]\n",
        "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
        "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_title('Validation F1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea05615-3aa9-4b17-89ee-40ef2eff0bcf",
      "metadata": {
        "id": "cea05615-3aa9-4b17-89ee-40ef2eff0bcf"
      },
      "source": [
        "# 3. Conclusion (5 Points)\n",
        "\n",
        "Provide an analysis for all the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa38684-e334-4958-8fe9-ac98cfbc7133",
      "metadata": {
        "id": "efa38684-e334-4958-8fe9-ac98cfbc7133"
      },
      "source": [
        "Answer: The accuracy and f1 scores of implementation and tensorflow are (59.13, 59.13) and (57.96, 57.96) respectively. The issue of less accuracy (loss not updating) might be in token generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhkPloxc-yEQ"
      },
      "id": "fhkPloxc-yEQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "47476cac-f0da-4b92-aa09-8f57e61053e9",
        "f05bdbd8-bd67-49b5-938a-a71d3dba105e",
        "436dfacd-2ad2-41fa-8f23-f4cc86142fe0"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}